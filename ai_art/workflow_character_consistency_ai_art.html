<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Workflow Guide: Character Consistency in AI Art (Midjourney + Adobe)</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
    <style>
        /* --- Base Styles & Variables --- */
        :root {
            --bg-color: #ffffff;
            --text-color: #212529;
            --muted-bg: #f8f9fa;
            --border-color: #e9ecef;
            --accent-color: #ea7a27;
            --accent-hover: #d96d1a;
            --header-bg: #212529;
            --header-fg: #f8f9fa;
            --neutral-gradient: linear-gradient(135deg, #f8f9fa, #e9ecef);
            --success-color: #16a34a;
            --success-bg: #dcfce7;
            --error-color: #dc2626;
            --error-bg: #fee2e2;
            --info-color: #0284c7;
            --info-bg: #e0f2fe;
        }

        *, *::before, *::after {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        @media (prefers-reduced-motion: reduce) {
            *, *::before, *::after {
                animation-duration: 0.01ms !important;
                animation-iteration-count: 1 !important;
                transition-duration: 0.01ms !important;
                scroll-behavior: auto !important;
            }
        }

        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
        }

        .container {
            max-width: 960px;
            margin: 0 auto;
            padding: 1rem;
        }
        
        /* --- Header & Footer --- */
        .site-header {
            background: var(--header-bg);
            color: var(--header-fg);
            padding: 14px 18px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            position: sticky;
            top: 0;
            z-index: 60;
        }
        .logo {
            font-weight: 700;
            font-size: 1.15rem;
            color: #fff;
            text-decoration: none;
        }
        .site-header nav {
            display: flex;
            gap: 14px;
        }
        .site-header nav a {
            color: #ced4da;
            text-decoration: none;
        }
        .site-header nav a:hover {
            color: #fff;
            text-decoration: underline;
        }
        .site-footer {
            background: var(--header-bg);
            color: var(--header-fg);
            padding: 14px 18px;
            text-align: center;
            margin-top: 2rem;
        }
        
        /* --- Breadcrumbs & Intro --- */
        .breadcrumbs {
            margin: 1.5rem 0 1rem;
            color: #6c757d;
            font-size: 0.9rem;
        }
        .breadcrumbs a {
            color: var(--accent-color);
            text-decoration: none;
        }
        .breadcrumbs a:hover {
            text-decoration: underline;
        }
        .intro {
            font-size: 1.05rem;
            color: #495057;
            margin-bottom: 2rem;
            text-align: center;
        }

        h1, h2, h3 {
            font-weight: 800;
            margin-bottom: 1rem;
        }
        
        h1 {
            text-align: center;
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }

        h2 {
            font-size: 1.75rem;
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 0.5rem;
            margin-top: 2.5rem;
        }
        
        section {
            margin-bottom: 2rem;
        }

        /* --- Workflow Pair Card --- */
        .confusion-pair-card {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1rem;
            background: var(--muted-bg);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }

        .pair-item h3 {
            color: var(--accent-color);
            font-weight: 600;
        }

        .formula {
            font-family: monospace;
            background: #e9ecef;
            padding: 0.5rem;
            border-radius: 6px;
            display: inline-block;
            margin-top: 0.5rem;
            font-size: 0.9rem;
        }

        .confusion-trigger {
            grid-column: 1 / -1;
            margin-top: 1rem;
            padding: 1rem;
            background: var(--error-bg);
            color: var(--error-color);
            border-radius: 8px;
            border: 1px solid var(--error-color);
            text-align: center;
            font-weight: 600;
        }

        .info-box {
            grid-column: 1 / -1;
            margin-top: 1rem;
            padding: 1rem;
            background: var(--info-bg);
            color: var(--info-color);
            border-radius: 8px;
            border: 1px solid var(--info-color);
            font-weight: 600;
        }
        
        /* --- Comparison Table --- */
        .table-container {
            overflow-x: auto;
            border: 1px solid var(--border-color);
            border-radius: 12px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            min-width: 600px;
        }

        th, td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        thead th {
            background-color: var(--text-color);
            color: var(--bg-color);
            position: sticky;
            top: 0;
            z-index: 10;
        }
        
        thead .helper-text {
            font-weight: 400;
            font-size: 0.8rem;
            color: #ced4da;
            display: block;
        }

        tbody tr:last-child td {
            border-bottom: none;
        }

        tbody tr:nth-child(even) {
            background-color: var(--muted-bg);
        }

        /* --- Mnemonics --- */
        .mnemonics-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1rem;
        }

        .mnemonic-card {
            background: var(--neutral-gradient);
            padding: 1.5rem;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            text-align: center;
        }
        
        .mnemonic-card .analogy {
            font-size: 1.5rem;
            font-weight: 800;
        }
        
        .mnemonic-card .explanation {
            margin-top: 0.5rem;
            color: #495057;
        }

        /* --- Glossary Cloud --- */
        .glossary-cloud {
            display: flex;
            flex-wrap: wrap;
            gap: 0.75rem;
            justify-content: center;
        }

        .glossary-chip {
            position: relative;
            background-color: #fff;
            border: 1px solid var(--border-color);
            border-radius: 50px;
            padding: 0.75rem 1.25rem;
            font-weight: 600;
            cursor: pointer;
            box-shadow: 0 2px 6px rgba(0,0,0,0.05);
            transition: all 0.2s ease;
            min-height: 44px;
        }

        .glossary-chip:hover, .glossary-chip:focus {
            background-color: var(--accent-color);
            color: white;
            border-color: var(--accent-hover);
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        
        .tooltip {
            position: fixed;
            background-color: var(--text-color);
            color: white;
            padding: 0.75rem 1rem;
            border-radius: 8px;
            z-index: 100;
            opacity: 0;
            visibility: hidden;
            pointer-events: none;
            transition: opacity 0.2s, visibility 0.2s;
            max-width: 300px;
            font-size: 0.9rem;
        }
        
        .tooltip.visible {
            opacity: 1;
            visibility: visible;
        }

        /* --- Quick Quiz --- */
        .quiz-container {
            background: var(--muted-bg);
            padding: 1.5rem;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            margin-bottom: 1.5rem;
        }
        
        .quiz-question {
            font-weight: 600;
            font-size: 1.1rem;
            margin-bottom: 1rem;
        }

        .quiz-options {
            display: grid;
            gap: 0.75rem;
        }

        .quiz-option {
            display: block;
            width: 100%;
            text-align: left;
            padding: 1rem;
            border-radius: 8px;
            border: 2px solid var(--border-color);
            background-color: #fff;
            cursor: pointer;
            font-weight: 600;
            min-height: 44px;
            transition: border-color 0.2s;
        }
        
        .quiz-option:hover:not([disabled]) {
             border-color: var(--accent-color);
        }

        .quiz-option[data-correct="true"] {
            background-color: var(--success-bg);
            border-color: var(--success-color);
            color: var(--success-color);
        }

        .quiz-option[data-correct="false"].selected {
            background-color: var(--error-bg);
            border-color: var(--error-color);
            color: var(--error-color);
        }
        
        .quiz-feedback {
            margin-top: 1rem;
            padding: 1rem;
            border-radius: 8px;
            display: none;
        }
        
        .quiz-feedback.correct {
            background-color: var(--success-bg);
            border: 1px solid var(--success-color);
            display: block;
        }

        .quiz-feedback.incorrect {
            background-color: var(--error-bg);
            border: 1px solid var(--error-color);
            display: block;
        }
        
        /* --- FAQ Accordion --- */
        .faq-accordion {
            display: grid;
            gap: 0.5rem;
        }
        .faq-question {
            width: 100%;
            background-color: var(--muted-bg);
            border: 1px solid var(--border-color);
            padding: 1rem 1.25rem;
            border-radius: 8px;
            cursor: pointer;
            font-weight: 600;
            display: flex;
            justify-content: space-between;
            align-items: center;
            text-align: left;
            font-size: 1rem;
            font-family: 'Inter', sans-serif;
        }
        .faq-question::after {
            content: '+';
            font-size: 1.5rem;
            color: var(--accent-color);
            transition: transform 0.2s;
        }
        .faq-question[aria-expanded="true"]::after {
            transform: rotate(45deg);
        }
        .faq-answer {
            padding: 1.25rem;
            border: 1px solid var(--border-color);
            border-top: none;
            border-radius: 0 0 8px 8px;
            background-color: #fff;
        }

        /* --- Takeaway --- */
        .takeaway {
            text-align: center;
            font-size: 1.5rem;
            font-weight: 800;
            padding: 2rem;
            background: var(--text-color);
            color: var(--bg-color);
            border-radius: 12px;
        }

        /* --- Responsive Breakpoints --- */
        @media (min-width: 640px) {
            .confusion-pair-card {
                grid-template-columns: 1fr 1fr;
            }
            .mnemonics-grid {
                grid-template-columns: repeat(2, 1fr);
            }
            .glossary-cloud {
                display: grid;
                grid-template-columns: repeat(2, 1fr);
            }
        }
        
        @media (min-width: 960px) {
            .mnemonics-grid {
                grid-template-columns: repeat(3, 1fr);
            }
             .glossary-cloud {
                grid-template-columns: repeat(3, 1fr);
            }
        }
    </style>
</head>
<body>
    <header class="site-header">
        <a class="logo" href="#">Beyond Dictionary</a>
        <nav>
            <a href="#">Learning</a>
            <a href="#">Games</a>
            <a href="#">Blog</a>
        </nav>
    </header>

    <div class="container">
        <main>
            <nav class="breadcrumbs" aria-label="Breadcrumb">
                <a href="#">AI Art</a> › <a href="#">Workflow Guides</a> › <span>Character Consistency 2025</span>
            </nav>
            <h1>Retaining Character Identity in AI Art</h1>
            <p class="intro">
                Character consistency is the #1 challenge in AI art production. This comprehensive guide explains the 2025 state-of-the-art: Midjourney v7's Omni Reference for creators, Adobe Firefly Custom Models for enterprises, and the critical distinction between individual workflows and production pipelines.
            </p>

            <!-- 1. Core Problem Statement -->
            <section aria-labelledby="problem-title">
                <h2 id="problem-title">The Challenge: Why Characters Drift</h2>
                <div class="confusion-pair-card">
                    <div class="pair-item">
                        <h3>Text Prompts Only (The Problem)</h3>
                        <p>Describing a character in words creates a different interpretation every time. "Blue-haired warrior with green eyes" could match millions of faces. AI models are designed for variety, not consistency—each generation is an independent creative act.</p>
                        <div class="formula">Result: 60-80% character drift rate</div>
                    </div>
                    <div class="pair-item">
                        <h3>Visual Anchors (The Solution)</h3>
                        <p>Reference images or trained models lock in a character's "DNA"—the specific visual identity that persists across scenes. The AI sees exactly who this person is, not just what they look like generically.</p>
                        <div class="formula">Result: 5-20% drift with proper setup</div>
                    </div>
                    <div class="confusion-trigger">
                        <strong>Key Insight:</strong> AI models can't "remember" characters between generations. You must provide that memory via references, trained models, or manual corrections.
                    </div>
                </div>
            </section>

            <!-- 2. Midjourney v7 Solutions -->
            <section aria-labelledby="midjourney-title">
                <h2 id="midjourney-title">Midjourney v7: Omni Reference & Style Reference</h2>
                <div class="confusion-pair-card">
                    <div class="pair-item">
                        <h3>--oref (Omni Reference)</h3>
                        <p>V7's character consistency feature. Upload one reference image showing your character, and Midjourney embeds that character's visual identity into new generations. Works with external photos or Midjourney-generated images.</p>
                        <div class="formula">--oref [URL] --ow 100</div>
                    </div>
                    <div class="pair-item">
                        <h3>--sref (Style Reference)</h3>
                        <p>Controls artistic style (colors, textures, mood) without affecting character identity. Combine with --oref to get consistent characters in different artistic styles. Can use images or preset style codes.</p>
                        <div class="formula">--sref [URL] --sw 100</div>
                    </div>
                    <div class="info-box">
                        <strong>Version Note:</strong> --cref (Character Reference) is v6.1 only. In v7, use --oref instead. Omni Reference is smarter, handles camera angles better, and costs 2x GPU time but delivers superior consistency.
                    </div>
                </div>
            </section>

            <!-- 3. Parameter Deep Dive Table -->
            <section aria-labelledby="parameters-title">
                <h2 id="parameters-title">Midjourney Parameters Explained</h2>
                 <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Parameter <span class="helper-text">What it does</span></th>
                                <th>Range</th>
                                <th>Default</th>
                                <th>Use For</th>
                                <th>Version</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>--oref</strong></td>
                                <td>Image URL</td>
                                <td>None</td>
                                <td>Character/object consistency in new scenes</td>
                                <td>v7 only</td>
                            </tr>
                            <tr>
                                <td><strong>--ow</strong></td>
                                <td>1-1000</td>
                                <td>100</td>
                                <td>Control how strictly to match reference. 50-250 recommended for most use. Higher values (400+) can cause artifacts.</td>
                                <td>v7 only</td>
                            </tr>
                            <tr>
                                <td><strong>--cref</strong></td>
                                <td>Image URL(s)</td>
                                <td>None</td>
                                <td>Character consistency (legacy feature)</td>
                                <td>v6.1 only</td>
                            </tr>
                            <tr>
                                <td><strong>--cw</strong></td>
                                <td>0-100</td>
                                <td>100</td>
                                <td>100 = face+hair+clothes. 0 = face only (change outfits). 60-80 for clothing variations.</td>
                                <td>v6.1 only</td>
                            </tr>
                            <tr>
                                <td><strong>--sref</strong></td>
                                <td>Image URL or code</td>
                                <td>None</td>
                                <td>Apply artistic style (not character identity). Use with --oref for styled characters.</td>
                                <td>v6, v7</td>
                            </tr>
                            <tr>
                                <td><strong>--sw</strong></td>
                                <td>0-1000</td>
                                <td>100</td>
                                <td>Style strength. Lower for subtle styling, higher to force specific aesthetic.</td>
                                <td>v6, v7</td>
                            </tr>
                            <tr>
                                <td><strong>--seed</strong></td>
                                <td>0-4294967295</td>
                                <td>Random</td>
                                <td>Experimental only. NOT recommended for consistency. Use --oref/--sref instead.</td>
                                <td>All versions</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- 4. Adobe Firefly Custom Models -->
            <section aria-labelledby="firefly-title">
                <h2 id="firefly-title">Adobe Firefly: Custom Models (Enterprise Solution)</h2>
                <div class="confusion-pair-card">
                    <div class="pair-item">
                        <h3>Custom Style Models</h3>
                        <p>Train Firefly on 10-30 brand images to learn colors, aesthetic, visual identity. Perfect for campaign consistency and brand guidelines at scale.</p>
                        <div class="formula">Concept: "Style1" → Brand aesthetic</div>
                    </div>
                    <div class="pair-item">
                        <h3>Custom Subject Models</h3>
                        <p>THE character solution. Train on 10-30 images of a specific character or object. Firefly learns that exact subject and can generate it in unlimited scenes. This is Adobe's answer to LoRA training.</p>
                        <div class="formula">Concept: "Alex" → Character DNA</div>
                    </div>
                    <div class="info-box">
                        <strong>Enterprise Only:</strong> Custom Models require Adobe enterprise license + Adobe Storage for business. Available via Firefly API, GenStudio, Express. NOT available to individual Photoshop users.
                    </div>
                </div>
            </section>

            <!-- 5. Solution Comparison Table -->
            <section aria-labelledby="solutions-title">
                <h2 id="solutions-title">Complete Solution Comparison</h2>
                 <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Solution <span class="helper-text">Different approaches</span></th>
                                <th>Best For</th>
                                <th>Consistency Level</th>
                                <th>Setup Time</th>
                                <th>Cost Tier</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Text Prompts Only</strong></td>
                                <td>Quick ideation, exploring variations</td>
                                <td>Low (60-80% drift)</td>
                                <td>Instant</td>
                                <td>$</td>
                            </tr>
                            <tr>
                                <td><strong>Midjourney v7 --oref</strong></td>
                                <td>Individual creators, freelancers, 5-100 images</td>
                                <td>High (10-20% drift)</td>
                                <td>5 minutes</td>
                                <td>$$</td>
                            </tr>
                            <tr>
                                <td><strong>Midjourney v6.1 --cref</strong></td>
                                <td>Legacy workflows, simpler character needs</td>
                                <td>Medium-High (15-30% drift)</td>
                                <td>5 minutes</td>
                                <td>$$</td>
                            </tr>
                            <tr>
                                <td><strong>Stable Diffusion LoRA</strong></td>
                                <td>Tech-savvy users, open-source workflows</td>
                                <td>Very High (5-10% drift)</td>
                                <td>2-4 hours</td>
                                <td>$ (GPU costs)</td>
                            </tr>
                            <tr>
                                <td><strong>Firefly Custom Subject Model</strong></td>
                                <td>Enterprise, brands, 100+ image campaigns</td>
                                <td>Very High (5-10% drift)</td>
                                <td>1-2 hours</td>
                                <td>$$$$ (Enterprise)</td>
                            </tr>
                            <tr>
                                <td><strong>Hybrid (MJ + Photoshop)</strong></td>
                                <td>Professional projects, IP licensing, publishing</td>
                                <td>Near-Perfect (0-5% final)</td>
                                <td>Variable</td>
                                <td>$$$</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- 6. Practical Examples -->
            <section aria-labelledby="examples-title">
                <h2 id="examples-title">Practical Workflow Examples</h2>
                <div class="mnemonics-grid">
                    <div class="mnemonic-card">
                        <div class="analogy">Solo Creator Workflow</div>
                        <p class="explanation">Generate character in MJ v7 → Save as reference → Use --oref for all new scenes → --ow 100-150 for tight consistency → Photoshop touchups as needed.</p>
                    </div>
                    <div class="mnemonic-card">
                        <div class="analogy">Style Variation Workflow</div>
                         <p class="explanation">Character ref: --oref [char.jpg] --ow 150 → Style ref: --sref [style.jpg] --sw 100 → Prompt: "Alex in cyberpunk alley" → Result: Same character, new style.</p>
                    </div>
                    <div class="mnemonic-card">
                        <div class="analogy">Enterprise Campaign</div>
                        <p class="explanation">Train Custom Subject Model on 20 mascot images → Deploy via Firefly API → Marketing team generates 500 on-brand variations → Automatic brand consistency.</p>
                    </div>
                    <div class="mnemonic-card">
                        <div class="analogy">Multi-Character Scene</div>
                        <p class="explanation">Generate each character separately with --oref → Export to Photoshop → Composite using layers → Generative Fill to blend → Manual refinement for perfection.</p>
                    </div>
                    <div class="mnemonic-card">
                        <div class="analogy">Changing Outfits</div>
                        <p class="explanation">V6.1: --cref [character.jpg] --cw 0 (face only) → V7: Lower --ow to 50-80 → Specify new outfit in prompt → Face stays same, clothes change.</p>
                    </div>
                    <div class="mnemonic-card">
                        <div class="analogy">Publication Workflow</div>
                        <p class="explanation">MJ generates base → Select best 3 variants → Photoshop: fix hands, eyes, consistency → Layer masks to composite best features → Upscale to print resolution.</p>
                    </div>
                </div>
            </section>

            <!-- 7. Glossary Cloud -->
            <section aria-labelledby="glossary-title">
                <h2 id="glossary-title">Essential Terminology</h2>
                <div class="glossary-cloud" id="glossaryCloud">
                    <!-- Chips will be inserted by JS -->
                </div>
                <div class="tooltip" id="tooltip"></div>
            </section>

            <!-- 8. Multiple Quizzes -->
            <section aria-labelledby="quiz-title">
                <h2 id="quiz-title">Test Your Knowledge</h2>
                
                <div class="quiz-container" id="quizContainer1">
                    <p class="quiz-question">You're using Midjourney v7 and want maximum character consistency. Which parameter combination is correct?</p>
                    <div class="quiz-options">
                        <button class="quiz-option" data-correct="false">--cref [URL] --cw 100</button>
                        <button class="quiz-option" data-correct="true">--oref [URL] --ow 100</button>
                        <button class="quiz-option" data-correct="false">--sref [URL] --sw 100</button>
                    </div>
                    <div class="quiz-feedback" id="quizFeedback1" aria-live="polite"></div>
                </div>

                <div class="quiz-container" id="quizContainer2">
                    <p class="quiz-question">An enterprise wants to generate 500 images of their mascot character across different campaigns. What's the best solution?</p>
                    <div class="quiz-options">
                        <button class="quiz-option" data-correct="false">Midjourney --oref (too manual for scale)</button>
                        <button class="quiz-option" data-correct="true">Firefly Custom Subject Model (enterprise-grade automation)</button>
                        <button class="quiz-option" data-correct="false">Photoshop Generative Fill (no character training capability)</button>
                    </div>
                    <div class="quiz-feedback" id="quizFeedback2" aria-live="polite"></div>
                </div>

                <div class="quiz-container" id="quizContainer3">
                    <p class="quiz-question">Why does Midjourney recommend AGAINST using --seed for character consistency?</p>
                    <div class="quiz-options">
                        <button class="quiz-option" data-correct="false">Seeds don't work at all in newer versions</button>
                        <button class="quiz-option" data-correct="true">Seeds are unpredictable and only affect initial noise, not actual character features</button>
                        <button class="quiz-option" data-correct="false">Seeds cost extra GPU time</button>
                    </div>
                    <div class="quiz-feedback" id="quizFeedback3" aria-live="polite"></div>
                </div>

                <div class="quiz-container" id="quizContainer4">
                    <p class="quiz-question">You want the same character in both photorealistic and anime styles. Which approach works?</p>
                    <div class="quiz-options">
                        <button class="quiz-option" data-correct="true">--oref for character + --sref for style in separate generations</button>
                        <button class="quiz-option" data-correct="false">--seed alone will maintain character across styles</button>
                        <button class="quiz-option" data-correct="false">One generation with both styles specified in text</button>
                    </div>
                    <div class="quiz-feedback" id="quizFeedback4" aria-live="polite"></div>
                </div>
            </section>
            
            <!-- 9. Comprehensive FAQ -->
            <section aria-labelledby="faq-title">
                <h2 id="faq-title">Frequently Asked Questions</h2>
                <div class="faq-accordion" id="faqAccordion">
                    <!-- FAQs will be inserted by JS -->
                </div>
            </section>

            <!-- 10. Takeaway -->
            <section aria-labelledby="takeaway-title">
                 <h2 id="takeaway-title" class="sr-only" style="display:none;">Key Takeaway</h2>
                <div class="takeaway">
                    Individual creators: Use Midjourney v7 --oref + Photoshop refinement. Enterprises: Train Firefly Custom Subject Models. Both need visual anchors—text alone never works.
                </div>
            </section>
        </main>
    </div>
    
    <footer class="site-footer">
        <small>© 2025 Beyond Dictionary | Updated October 2025</small>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // --- Glossary Cloud ---
            const glossaryData = [
                { term: 'Omni Reference (--oref)', definition: 'Midjourney v7 feature that embeds a character or object from a reference image into new generations. Replaces v6 character reference. Parameter: --oref [URL]' },
                { term: 'Omni Weight (--ow)', definition: 'Controls strength of omni reference, 1-1000, default 100. Range 50-250 recommended. Higher values can cause artifacts. Use lower for style changes.' },
                { term: 'Character Reference (--cref)', definition: 'Midjourney v6.1 feature for character consistency. Deprecated in v7—use --oref instead. Can reference multiple images: --cref URL1 URL2' },
                { term: 'Character Weight (--cw)', definition: 'V6.1 parameter 0-100. Default 100 includes face+hair+clothes. Set to 0 for face only (allows outfit changes). 60-80 for clothing variations.' },
                { term: 'Style Reference (--sref)', definition: 'References artistic style without affecting character identity. Works in v6 and v7. Can use image URLs or preset style codes. Combine with --oref.' },
                { term: 'Style Weight (--sw)', definition: 'Controls style reference strength, 0-1000, default 100. Independent of character consistency. Use high values to force specific aesthetic.' },
                { term: 'Seed (--seed)', definition: 'Random noise starting point for generation (0-4294967295). NOT recommended for consistency—unpredictable behavior. Use references instead.' },
                { term: 'Custom Subject Model', definition: 'Adobe Firefly enterprise feature. Train on 10-30 images of a character/object. Firefly learns exact subject for unlimited consistent generations. Requires enterprise license.' },
                { term: 'Custom Style Model', definition: 'Adobe Firefly model trained on brand aesthetic (colors, shapes, composition). Different from subject models—learns style, not specific objects/characters.' },
                { term: 'Character DNA', definition: 'Metaphor for core visual identity that persists across generations. Established via reference images or trained models, not text prompts.' },
                { term: 'Character Drift', definition: 'When AI generates variations of a character that don\'t match previous versions. Caused by text-only prompts without visual anchors. Rate: 60-80% without references.' },
                { term: 'LoRA (Low-Rank Adaptation)', definition: 'Technique for fine-tuning Stable Diffusion on 10-20 character images. Not usable in Midjourney but achieves similar results to Firefly Custom Models.' },
                { term: 'Generative Fill', definition: 'Photoshop AI feature for filling selected regions. Can use reference images for guidance but lacks character training—manual workflow required for consistency.' },
                { term: 'Prompt Engineering', definition: 'Crafting detailed, consistent text descriptions. Critical for character work: include same distinctive features in every prompt. But text alone = drift.' },
                { term: 'Iterative Refinement', definition: 'Generate → Evaluate → Fix → Repeat. Essential workflow for consistency. Generate in MJ, refine in PS, use result as new reference.' },
                { term: 'Non-Destructive Editing', definition: 'Photoshop workflow using layers and masks. Preserves originals while testing AI edits. Critical for mixing multiple generations into one consistent image.' },
                { term: 'Multi-Character Consistency', definition: 'Advanced challenge. Solutions: (1) Generate separately + composite in PS. (2) Use multiple --oref images. (3) Enterprise: train separate Custom Models.' },
                { term: 'Style Code (sref code)', definition: 'Numerical preset in Midjourney\'s style library (e.g., --sref 19961010). Use --sref random to discover new codes. Style Explorer in web UI.' }
            ];

            const glossaryCloud = document.getElementById('glossaryCloud');
            const tooltip = document.getElementById('tooltip');

            glossaryData.forEach(item => {
                const chip = document.createElement('button');
                chip.className = 'glossary-chip';
                chip.textContent = item.term;
                chip.dataset.definition = item.definition;
                glossaryCloud.appendChild(chip);
            });
            
            let activeChip = null;

            glossaryCloud.addEventListener('click', function(e) {
                if (e.target.classList.contains('glossary-chip')) {
                    const chip = e.target;
                    
                    if (activeChip === chip) {
                        hideTooltip();
                        return;
                    }
                    
                    activeChip = chip;
                    tooltip.textContent = chip.dataset.definition;
                    positionAndShowTooltip(chip);
                }
            });
            
            function positionAndShowTooltip(chip) {
                const chipRect = chip.getBoundingClientRect();
                tooltip.classList.add('visible');
                const tooltipRect = tooltip.getBoundingClientRect();
                
                let top = chipRect.bottom + 8;
                let left = chipRect.left + (chipRect.width / 2) - (tooltipRect.width / 2);

                if (left < 10) left = 10;
                if (left + tooltipRect.width > window.innerWidth - 10) {
                    left = window.innerWidth - tooltipRect.width - 10;
                }
                if (top + tooltipRect.height > window.innerHeight - 10) {
                    top = chipRect.top - tooltipRect.height - 8;
                }

                tooltip.style.top = `${top}px`;
                tooltip.style.left = `${left}px`;
            }

            function hideTooltip() {
                tooltip.classList.remove('visible');
                activeChip = null;
            }

            window.addEventListener('scroll', hideTooltip);
            document.addEventListener('click', function(e) {
                if (activeChip && !activeChip.contains(e.target)) {
                    hideTooltip();
                }
            });

            // --- Quiz System ---
            function initQuiz(containerId, feedbackId, feedbackTexts) {
                const container = document.getElementById(containerId);
                const options = container.querySelectorAll('.quiz-option');
                const feedbackEl = document.getElementById(feedbackId);
                let answered = false;

                options.forEach(option => {
                    option.addEventListener('click', function() {
                        if (answered) return;
                        answered = true;

                        const isCorrect = this.dataset.correct === 'true';
                        this.classList.add('selected');

                        if (isCorrect) {
                            feedbackEl.textContent = feedbackTexts.correct;
                            feedbackEl.className = 'quiz-feedback correct';
                        } else {
                            feedbackEl.textContent = feedbackTexts.incorrect;
                            feedbackEl.className = 'quiz-feedback incorrect';
                        }
                        
                        options.forEach(opt => opt.disabled = true);
                    });
                });
            }

            initQuiz('quizContainer1', 'quizFeedback1', {
                correct: 'Correct! In Midjourney v7, --oref (Omni Reference) replaced --cref. Use --ow to control strength. --cref only works in v6.1.',
                incorrect: 'Not quite. --cref is v6.1 only, and --sref controls style (not character). In v7, use --oref for character consistency with --ow for weight control.'
            });

            initQuiz('quizContainer2', 'quizFeedback2', {
                correct: 'Correct! Firefly Custom Subject Models are designed for enterprise-scale character consistency. Train once, generate unlimited variations via API. Midjourney --oref works but requires manual reference attachment each time.',
                incorrect: 'At enterprise scale (500+ images), manual workflows break down. Custom Subject Models automate consistency via trained models accessible through APIs. Midjourney works for small batches; Photoshop lacks character training entirely.'
            });

            initQuiz('quizContainer3', 'quizFeedback3', {
                correct: 'Correct! Midjourney documentation explicitly states: "For consistency in your images, we recommend style references, character references, and personalization. Seeds may behave unexpectedly." Seeds only control initial visual noise, not character features.',
                incorrect: 'Seeds control the starting point (visual noise field) but don\'t guarantee character consistency. They\'re unpredictable across sessions and have minimal impact on actual features. Midjourney recommends --oref and --sref instead.'
            });

            initQuiz('quizContainer4', 'quizFeedback4', {
                correct: 'Correct! Use --oref for character consistency and --sref to apply different styles in separate generations. Example: "portrait of Alex --oref char.jpg --sref anime_style.jpg" then separately with photorealistic style reference.',
                incorrect: 'You need separate parameters: --oref locks character identity while --sref changes artistic style. Generate twice with different style references but the same character reference. Seeds and text alone won\'t maintain consistency.'
            });

            // --- FAQ Section ---
            const faqData = [
                { 
                    q: 'What\'s the actual difference between --oref and --cref?', 
                    a: '<strong>Version compatibility:</strong> --cref works in v6.1, --oref works in v7. <strong>Intelligence:</strong> Omni Reference (--oref) handles camera angles and complex poses better than the older character reference. <strong>Flexibility:</strong> --oref works well with external photos; --cref was optimized for Midjourney-generated images. <strong>Cost:</strong> --oref uses 2x GPU time. <strong>Bottom line:</strong> If you\'re on v7, use --oref. If you\'re still on v6.1, use --cref. They\'re not compatible across versions.' 
                },
                { 
                    q: 'Can individual Photoshop users access Firefly Custom Models?', 
                    a: 'No. Custom Models (both Style and Subject) require an Adobe enterprise license + Adobe Storage for business. They\'re available through Firefly Services API, GenStudio for Performance Marketing, and Adobe Express for enterprise teams. Individual Creative Cloud subscribers cannot train custom models. <strong>Workaround for individuals:</strong> Use Midjourney --oref for generation, then Photoshop Generative Fill with reference images for refinement. It\'s manual but effective for small projects.' 
                },
                { 
                    q: 'How many reference images should I use for training?', 
                    a: '<strong>Firefly Custom Subject Models:</strong> 10-30 images showing variety in angles, lighting, and contexts. More diversity = better generalization. <strong>Stable Diffusion LoRA:</strong> Similar range, 10-20 high-quality images. <strong>Midjourney --oref:</strong> Just 1 image as reference (but having a character sheet with multiple angles helps). <strong>Quality matters:</strong> Clear, high-resolution images beat quantity. Avoid blurry, pixelated, or heavily filtered photos.' 
                },
                { 
                    q: 'Why does my character still look different even with --oref at maximum weight?', 
                    a: 'Several factors compete with references: <strong>(1) High stylize values (--s):</strong> Creativity overrides consistency. Lower --s or increase --ow to compensate. <strong>(2) Conflicting prompts:</strong> If your prompt describes features that contradict the reference, AI gets confused. <strong>(3) Dramatic style changes:</strong> Going from photo to anime requires lower --ow (25-50) and reinforcing key features in text. <strong>(4) Reference quality:</strong> Cluttered backgrounds or multiple subjects dilute focus—crop tightly. <strong>(5) Composition conflicts:</strong> Requesting impossible poses may force drift. Test systematically by changing one variable at a time.' 
                },
                { 
                    q: 'How do I handle multiple characters in one scene?', 
                    a: '<strong>Method 1 - Composite workflow (most reliable):</strong> Generate each character separately with their --oref → Export to Photoshop → Layer each character → Use Generative Fill to create unified background → Blend with masks. <strong>Method 2 - Multi-reference (experimental):</strong> Some users report success with side-by-side reference images in one file, but control is limited. <strong>Method 3 - Sequential generation:</strong> Generate main character with --oref, then use that result as style reference while generating second character. <strong>Enterprise:</strong> Train separate Custom Subject Models for each character, generate individually, composite in production workflow.' 
                },
                { 
                    q: 'What\'s the workflow for changing character outfits while keeping the face?', 
                    a: '<strong>Midjourney v6.1:</strong> Use --cref [character.jpg] --cw 0 (zero weight focuses on face only, ignores clothing). Describe new outfit in prompt. <strong>Midjourney v7:</strong> Use --oref [character.jpg] --ow 50-80 (lower weight gives flexibility). Prompt: "Alex wearing red dress" → face stays consistent, clothes change. <strong>Photoshop method:</strong> Generate character in base outfit → Use Generative Fill to select clothing area → Reference image of new outfit → Specify "outfit change" in prompt. <strong>Pro tip:</strong> Describe facial features explicitly even when changing clothes ("Alex with short black hair, green eyes, wearing red dress") to reinforce identity.' 
                },
                { 
                    q: 'Should I use seed for character consistency instead of references?', 
                    a: 'No. Midjourney explicitly recommends against this. From their documentation: "For consistency in your images, we recommend style references, character references, and personalization. Seeds may behave unexpectedly and shouldn\'t be relied upon." <strong>Why seeds don\'t work:</strong> Seeds only control initial visual noise (like TV static), not actual features. They affect composition and layout but not identity. Behavior is unpredictable across sessions. <strong>What seeds ARE useful for:</strong> Testing prompt variations with similar composition. Generating multiple angles of the same scene. Experimenting with parameter changes while keeping layout consistent. <strong>For characters:</strong> Always use --oref or --sref, never rely on --seed alone.' 
                },
                { 
                    q: 'How does style reference (--sref) interact with character reference (--oref)?', 
                    a: 'They\'re complementary and can be combined: <strong>--oref:</strong> Locks character identity (who the person is). <strong>--sref:</strong> Applies artistic style (how the image looks). <strong>Combined example:</strong> "portrait of Alex --oref character.jpg --ow 150 --sref watercolor.jpg --sw 100" → Same character, watercolor style. <strong>Weight balancing:</strong> If style overwhelms character, increase --ow or decrease --sw. If character is too rigid, decrease --ow. <strong>Reinforce in text:</strong> When applying strong styles, mention key character features in prompt: "watercolor portrait of young woman with blue hair, green eyes" helps AI maintain identity through style transformation. <strong>Experimentation:</strong> Try different weight combinations. Each character/style pairing behaves differently.' 
                },
                { 
                    q: 'What\'s the difference between Firefly Custom Style vs Custom Subject models?', 
                    a: '<strong>Custom Style Model:</strong> Learns aesthetic—colors, lighting, composition, mood. Doesn\'t lock specific objects/people. Use for brand consistency, campaign visual identity. Keyword: abstract concept ("Style1"). <strong>Custom Subject Model:</strong> Learns specific character or object. Maintains exact identity across generations. Use for mascots, characters, product shots. Keyword: proper noun ("Alex", "Mascot"). <strong>Training data:</strong> Style models need varied subjects showing consistent aesthetic. Subject models need same subject in varied contexts. <strong>Use together:</strong> Train style model for brand look + subject model for character → On-brand character generations. <strong>Example:</strong> E-commerce brand trains style model on product photography aesthetic, subject models on each product → Generate any product in brand style automatically.' 
                },
                { 
                    q: 'Can I train a Firefly Custom Model on AI-generated images?', 
                    a: 'Yes, and it\'s a popular technique. Workflow: <strong>(1)</strong> Generate 15-20 character images in Midjourney using --oref for consistency. <strong>(2)</strong> Curate best images showing variety (angles, expressions, lighting). <strong>(3)</strong> Upload to Firefly Custom Model training (requires enterprise license). <strong>(4)</strong> Train Custom Subject Model. <strong>(5)</strong> Now unlimited generations via Firefly API. <strong>Benefits:</strong> Midjourney creates initial aesthetic, Firefly locks it in for production scale. Combines creative exploration with industrial consistency. <strong>Quality tip:</strong> Ensure training set has variety but maintains recognizable identity. Don\'t use drastically different images—AI won\'t know what features to preserve.' 
                },
                { 
                    q: 'How do I choose between Midjourney and Firefly for my project?', 
                    a: '<strong>Use Midjourney if:</strong> You\'re an individual creator or small team. Need 5-100 images. Want maximum creative flexibility. Budget is moderate. Comfortable with iteration. <strong>Use Firefly Custom Models if:</strong> You\'re an enterprise with licensing. Need 100+ consistent images. Require automated workflows via API. Budget supports enterprise tools. Need legal indemnification for commercial use. <strong>Hybrid approach:</strong> Many agencies use Midjourney for creative exploration → Select winning concepts → Train Firefly Custom Models for production → Scale via API. <strong>Budget reality:</strong> Midjourney = $30-120/month. Firefly Custom Models = enterprise pricing (contact sales). Most solo creators use MJ + Photoshop; enterprises use Firefly.' 
                },
                { 
                    q: 'My Photoshop Generative Fill keeps changing my character\'s face. How do I fix this?', 
                    a: 'Photoshop Generative Fill doesn\'t have built-in character training, so you need manual techniques: <strong>(1) Use Reference Images:</strong> In Generative Fill panel, upload your character as reference image—helps but not perfect. <strong>(2) Select precisely:</strong> Only fill areas you want changed (background, clothing). Avoid face unless specifically fixing it. <strong>(3) Layer masks:</strong> Generate multiple fills, use masks to composite best parts. <strong>(4) Lower generation area:</strong> Small selections = less chance of changing character. <strong>(5) Detailed prompts:</strong> "Maintain character with short black hair and green eyes, change only background." <strong>Reality check:</strong> Consumer Photoshop isn\'t designed for character consistency—it\'s a refinement tool, not a character engine. Use Midjourney for character generation, Photoshop for final touchups.' 
                }
            ];
            
            const faqAccordion = document.getElementById('faqAccordion');

            faqData.forEach(item => {
                const faqItem = document.createElement('div');
                faqItem.className = 'faq-item';
                
                const question = document.createElement('button');
                question.className = 'faq-question';
                question.setAttribute('aria-expanded', 'false');
                question.innerHTML = item.q;

                const answer = document.createElement('div');
                answer.className = 'faq-answer';
                answer.hidden = true;
                answer.innerHTML = `<p>${item.a}</p>`;
                
                faqItem.appendChild(question);
                faqItem.appendChild(answer);
                faqAccordion.appendChild(faqItem);
            });

            faqAccordion.addEventListener('click', e => {
                const question = e.target.closest('.faq-question');
                if (!question) return;

                const isExpanded = question.getAttribute('aria-expanded') === 'true';
                const answer = question.nextElementSibling;
                
                question.setAttribute('aria-expanded', !isExpanded);
                answer.hidden = isExpanded;
            });
        });
    </script>
</body>
</html>